{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Minip.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebG3C2o3INAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec1fb3a-ee4f-42a9-b809-ddba8c81b1c2"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import nltk\n",
        "import string\n",
        "import sklearn\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rFbR8giJJkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ca0897-fb89-4a23-e49d-1f7b1591bde2"
      },
      "source": [
        "df=pd.read_csv(r'IMDB Dataset.csv')\n",
        "dataset=df[:10000]\n",
        "toke=[]\n",
        "def clean_text(dataset):\n",
        "    clean_dataset=list()\n",
        "    lines=dataset['review'].values.tolist()\n",
        "    for text in lines:\n",
        "        text=text.lower()\n",
        "        pattern=re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "        text=pattern.sub('',text)\n",
        "        text=re.sub(r\"[,','.\\\"!@#$%^&*(){}?/;`~:<>+=-_]\",\"\",text)\n",
        "        tokens=word_tokenize(text)\n",
        "        # print(tokens)\n",
        "        table=str.maketrans('','',string.punctuation)\n",
        "        stripped=[w.translate(table)for w in tokens]\n",
        "        words=[word for word in stripped if word.isalpha()]\n",
        "        words=' '.join(words)\n",
        "        clean_dataset.append(words)\n",
        "\n",
        "    return clean_dataset\n",
        "clean_dataset=clean_text(dataset)\n",
        "wordnet= WordNetLemmatizer()\n",
        "lemmetized_words=[]\n",
        "for i in range(len(clean_dataset)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', clean_dataset[i])\n",
        "    review= review.split()\n",
        "    review= [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    # review= ' '.join(review)\n",
        "    lemmetized_words.append(review)\n",
        "# print(lemmetized_words[:5])\n",
        "for i in range(10):\n",
        "  print(lemmetized_words[i])\n",
        "\n",
        "vocab_size= 1000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['one', 'reviewer', 'mentioned', 'watching', 'oz', 'episode', 'youll', 'hooked', 'right', 'exactly', 'happened', 'mebr', 'br', 'first', 'thing', 'struck', 'oz', 'brutality', 'unflinching', 'scene', 'violence', 'set', 'right', 'word', 'go', 'trust', 'show', 'faint', 'hearted', 'timid', 'show', 'pull', 'punch', 'regard', 'drug', 'sex', 'violence', 'hardcore', 'classic', 'use', 'wordbr', 'br', 'called', 'oz', 'nickname', 'given', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'focus', 'mainly', 'emerald', 'city', 'experimental', 'section', 'prison', 'cell', 'glass', 'front', 'face', 'inwards', 'privacy', 'high', 'agenda', 'em', 'city', 'home', 'manyaryans', 'muslim', 'gangsta', 'latino', 'christian', 'italian', 'irish', 'moreso', 'scuffle', 'death', 'stare', 'dodgy', 'dealing', 'shady', 'agreement', 'never', 'far', 'awaybr', 'br', 'would', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'go', 'show', 'wouldnt', 'dare', 'forget', 'pretty', 'picture', 'painted', 'mainstream', 'audience', 'forget', 'charm', 'forget', 'romanceoz', 'doesnt', 'mess', 'around', 'first', 'episode', 'ever', 'saw', 'struck', 'nasty', 'surreal', 'couldnt', 'say', 'ready', 'watched', 'developed', 'taste', 'oz', 'got', 'accustomed', 'high', 'level', 'graphic', 'violence', 'violence', 'injustice', 'crooked', 'guard', 'wholl', 'sold', 'nickel', 'inmate', 'wholl', 'kill', 'order', 'get', 'away', 'well', 'mannered', 'middle', 'class', 'inmate', 'turned', 'prison', 'bitch', 'due', 'lack', 'street', 'skill', 'prison', 'experience', 'watching', 'oz', 'may', 'become', 'comfortable', 'uncomfortable', 'viewingthats', 'get', 'touch', 'darker', 'side']\n",
            "['wonderful', 'little', 'production', 'br', 'br', 'filming', 'technique', 'unassuming', 'oldtimebbc', 'fashion', 'give', 'comforting', 'sometimes', 'discomforting', 'sense', 'realism', 'entire', 'piece', 'br', 'br', 'actor', 'extremely', 'well', 'chosen', 'michael', 'sheen', 'got', 'polari', 'voice', 'pat', 'truly', 'see', 'seamless', 'editing', 'guided', 'reference', 'williams', 'diary', 'entry', 'well', 'worth', 'watching', 'terrificly', 'written', 'performed', 'piece', 'masterful', 'production', 'one', 'great', 'master', 'comedy', 'life', 'br', 'br', 'realism', 'really', 'come', 'home', 'little', 'thing', 'fantasy', 'guard', 'rather', 'use', 'traditional', 'dream', 'technique', 'remains', 'solid', 'disappears', 'play', 'knowledge', 'sens', 'particularly', 'scene', 'concerning', 'orton', 'halliwell', 'set', 'particularly', 'flat', 'halliwells', 'mural', 'decorating', 'every', 'surface', 'terribly', 'well', 'done']\n",
            "['thought', 'wonderful', 'way', 'spend', 'time', 'hot', 'summer', 'weekend', 'sitting', 'air', 'conditioned', 'theater', 'watching', 'lighthearted', 'comedy', 'plot', 'simplistic', 'dialogue', 'witty', 'character', 'likable', 'even', 'well', 'bread', 'suspected', 'serial', 'killer', 'may', 'disappointed', 'realize', 'match', 'point', 'risk', 'addiction', 'thought', 'proof', 'woody', 'allen', 'still', 'fully', 'control', 'style', 'many', 'u', 'grown', 'lovebr', 'br', 'id', 'laughed', 'one', 'woodys', 'comedy', 'year', 'dare', 'say', 'decade', 'ive', 'never', 'impressed', 'scarlet', 'johanson', 'managed', 'tone', 'sexy', 'image', 'jumped', 'right', 'average', 'spirited', 'young', 'womanbr', 'br', 'may', 'crown', 'jewel', 'career', 'wittier', 'devil', 'wear', 'prada', 'interesting', 'superman', 'great', 'comedy', 'go', 'see', 'friend']\n",
            "['basically', 'there', 'family', 'little', 'boy', 'jake', 'think', 'there', 'zombie', 'closet', 'parent', 'fighting', 'timebr', 'br', 'movie', 'slower', 'soap', 'opera', 'suddenly', 'jake', 'decides', 'become', 'rambo', 'kill', 'zombiebr', 'br', 'ok', 'first', 'youre', 'going', 'make', 'film', 'must', 'decide', 'thriller', 'drama', 'drama', 'movie', 'watchable', 'parent', 'divorcing', 'arguing', 'like', 'real', 'life', 'jake', 'closet', 'totally', 'ruin', 'film', 'expected', 'see', 'boogeyman', 'similar', 'movie', 'instead', 'watched', 'drama', 'meaningless', 'thriller', 'spotsbr', 'br', 'well', 'playing', 'parent', 'descent', 'dialog', 'shot', 'jake', 'ignore']\n",
            "['petter', 'matteis', 'love', 'time', 'money', 'visually', 'stunning', 'film', 'watch', 'mr', 'mattei', 'offer', 'u', 'vivid', 'portrait', 'human', 'relation', 'movie', 'seems', 'telling', 'u', 'money', 'power', 'success', 'people', 'different', 'situation', 'encounter', 'br', 'br', 'variation', 'arthur', 'schnitzlers', 'play', 'theme', 'director', 'transfer', 'action', 'present', 'time', 'new', 'york', 'different', 'character', 'meet', 'connect', 'one', 'connected', 'one', 'way', 'another', 'next', 'person', 'one', 'seems', 'know', 'previous', 'point', 'contact', 'stylishly', 'film', 'sophisticated', 'luxurious', 'look', 'taken', 'see', 'people', 'live', 'world', 'live', 'habitatbr', 'br', 'thing', 'one', 'get', 'soul', 'picture', 'different', 'stage', 'loneliness', 'one', 'inhabits', 'big', 'city', 'exactly', 'best', 'place', 'human', 'relation', 'find', 'sincere', 'fulfillment', 'one', 'discerns', 'case', 'people', 'encounterbr', 'br', 'acting', 'good', 'mr', 'matteis', 'direction', 'steve', 'buscemi', 'rosario', 'dawson', 'carol', 'kane', 'michael', 'imperioli', 'adrian', 'grenier', 'rest', 'talented', 'cast', 'make', 'character', 'come', 'alivebr', 'br', 'wish', 'mr', 'mattei', 'good', 'luck', 'await', 'anxiously', 'next', 'work']\n",
            "['probably', 'alltime', 'favorite', 'movie', 'story', 'selflessness', 'sacrifice', 'dedication', 'noble', 'cause', 'preachy', 'boring', 'never', 'get', 'old', 'despite', 'seen', 'time', 'last', 'year', 'paul', 'lukas', 'performance', 'brings', 'tear', 'eye', 'bette', 'davis', 'one', 'truly', 'sympathetic', 'role', 'delight', 'kid', 'grandma', 'say', 'like', 'dressedup', 'midget', 'child', 'make', 'fun', 'watch', 'mother', 'slow', 'awakening', 'whats', 'happening', 'world', 'roof', 'believable', 'startling', 'dozen', 'thumb', 'theyd', 'movie']\n",
            "['sure', 'would', 'like', 'see', 'resurrection', 'dated', 'seahunt', 'series', 'tech', 'today', 'would', 'bring', 'back', 'kid', 'excitement', 'mei', 'grew', 'black', 'white', 'tv', 'seahunt', 'gunsmoke', 'hero', 'every', 'weekyou', 'vote', 'comeback', 'new', 'sea', 'huntwe', 'need', 'change', 'pace', 'tv', 'would', 'work', 'world', 'water', 'adventureoh', 'way', 'thank', 'outlet', 'like', 'view', 'many', 'viewpoint', 'tv', 'many', 'moviesso', 'ole', 'way', 'believe', 'ive', 'got', 'wan', 'na', 'saywould', 'nice', 'read', 'plus', 'point', 'sea', 'huntif', 'rhyme', 'would', 'line', 'would', 'let', 'submitor', 'leave', 'doubt', 'quitif', 'must', 'go', 'let']\n",
            "['show', 'amazing', 'fresh', 'innovative', 'idea', 'first', 'aired', 'first', 'year', 'brilliant', 'thing', 'dropped', 'show', 'really', 'funny', 'anymore', 'continued', 'decline', 'complete', 'waste', 'time', 'todaybr', 'br', 'truly', 'disgraceful', 'far', 'show', 'fallen', 'writing', 'painfully', 'bad', 'performance', 'almost', 'bad', 'mildly', 'entertaining', 'respite', 'guesthosts', 'show', 'probably', 'wouldnt', 'still', 'air', 'find', 'hard', 'believe', 'creator', 'handselected', 'original', 'cast', 'also', 'chose', 'band', 'hack', 'followed', 'one', 'recognize', 'brilliance', 'see', 'fit', 'replace', 'mediocrity', 'felt', 'must', 'give', 'star', 'respect', 'original', 'cast', 'made', 'show', 'huge', 'success', 'show', 'awful', 'cant', 'believe', 'still', 'air']\n",
            "['encouraged', 'positive', 'comment', 'film', 'looking', 'forward', 'watching', 'film', 'bad', 'mistake', 'ive', 'seen', 'film', 'truly', 'one', 'worst', 'awful', 'almost', 'every', 'way', 'editing', 'pacing', 'storyline', 'acting', 'soundtrack', 'film', 'song', 'lame', 'country', 'tune', 'played', 'le', 'four', 'time', 'film', 'look', 'cheap', 'nasty', 'boring', 'extreme', 'rarely', 'happy', 'see', 'end', 'credit', 'film', 'br', 'br', 'thing', 'prevents', 'giving', 'harvey', 'keitel', 'far', 'best', 'performance', 'least', 'seems', 'making', 'bit', 'effort', 'one', 'keitel', 'obsessive']\n",
            "['like', 'original', 'gut', 'wrenching', 'laughter', 'like', 'movie', 'young', 'old', 'love', 'movie', 'hell', 'even', 'mom', 'liked', 'itbr', 'br', 'great', 'camp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZZx7ICPNW_m",
        "outputId": "bf3f4489-546d-4173-9cec-d9910c36fbf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "le= LabelEncoder()\n",
        "xy= dataset['sentiment'].values.tolist()\n",
        "le.fit(['positive', 'negative'])\n",
        "new_xy=le.transform(xy)\n",
        "print(new_xy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 ... 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#splitting the Dataset into train and test set\n",
        "totalRows=np.shape(lemmetized_words)[0]\n",
        "\n",
        "splitRatio=0.75\n",
        "splitPoint=int(splitRatio*totalRows)\n",
        "\n",
        "trainReviews=lemmetized_words[:splitPoint]\n",
        "trainLabels=new_xy[:splitPoint]\n",
        "testReviews=lemmetized_words[splitPoint:]\n",
        "testLabels=new_xy[splitPoint:]\n"
      ],
      "metadata": {
        "id": "i_YzfmLaekk8",
        "outputId": "877a2410-05c8-4118-a275-65b70b16890e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:1970: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  result = asarray(a).shape\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nTRbowS0IDo",
        "outputId": "a2406566-9ed8-4e56-93b3-1f022f7fe486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "vectorizer = sklearn.feature_extraction.text.CountVectorizer(binary=False,ngram_range=(1,1))\n",
        "output = vectorizer.fit_transform(lemmetized_words)\n",
        "# tf_features_test = vectorizer.transform(imdb_test['review'])\n",
        "# xd= output.toarray()\n",
        "l=vectorizer.get_feature_names_out()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-3cdf1c35838d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmetized_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# tf_features_test = vectorizer.transform(imdb_test['review'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# xd= output.toarray()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \"\"\"\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#learning word embeddings on training data using Gensim library\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import nltk\n",
        "\n",
        "embeddingsSize=256\n",
        "model=Word2Vec(lemmetized_words, size=embeddingsSize, window=5, min_count=1, workers=4)\n",
        "######################Training of Word Embeddings Vector Completed#########\n",
        "import numpy as np\n",
        "def getVectors(dataset):\n",
        "  singleDataItemEmbedding=np.zeros(embeddingsSize)\n",
        "  vectors=[]\n",
        "  for dataItem in dataset:\n",
        "    wordCount=0\n",
        "    for word in dataItem:\n",
        "      if word in model.wv.vocab:\n",
        "        singleDataItemEmbedding=singleDataItemEmbedding+model.wv[word]\n",
        "        wordCount=wordCount+1\n",
        "  \n",
        "    singleDataItemEmbedding=singleDataItemEmbedding/wordCount  \n",
        "    vectors.append(singleDataItemEmbedding)\n",
        "  return vectors\n",
        "# lemmetizedwordsgetvectors=getVectors(lemmetized_words)\n",
        "trainReviewVectors=getVectors(trainReviews)\n",
        "testReviewVectors=getVectors(testReviews)"
      ],
      "metadata": {
        "id": "JDG5Xe4QFOFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp-We1dl1uJd"
      },
      "source": [
        "df= pd.DataFrame(lemmetizedwordsgetvectors)\n",
        "df[\"sentiments\"]=new_xy\n",
        "df.to_csv('DATASET_REVISED.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.DataFrame(trainReviewVectors)\n",
        "df[\"sentiments\"]=trainLabels\n",
        "df.to_csv('TRAIN_review.csv')"
      ],
      "metadata": {
        "id": "3YHMpASHjIdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.DataFrame(testReviewVectors)\n",
        "df[\"sentiments\"]=testLabels\n",
        "df.to_csv('TEST_review.csv')"
      ],
      "metadata": {
        "id": "it9BFDx0kXHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "#add path of google drive to environment variable to load python files from google drive\n",
        "import sys\n",
        "# sys.path.insert(1, \"/content/drive/My Drive/nlp assignments/assignment 4\")\n",
        "# from visualization import plot_confusion_matrix_from_data\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def printResults(y_true, y_predicted):\n",
        "  print(\"Accuracy= \", accuracy_score(y_true, y_predicted))\n",
        "\n",
        "  columns=['false', 'true']\n",
        "  # plot_confusion_matrix_from_data(y_true, y_predicted, columns)\n",
        "\n",
        "  precision, recall, fscore, support = score(y_true, y_predicted)\n",
        "\n",
        "  print('###########################################')\n",
        "  print('precision: {}'.format(precision))  \n",
        "  print('recall: {}'.format(recall))\n",
        "  print('fscore: {}'.format(fscore))\n",
        "  print('support: {}'.format(support))\n",
        "  print('###########################################3')\n",
        "\n",
        "  print('Macro F1 ',f1_score(y_true, y_predicted, average='macro'))\n",
        "\n",
        "  print('Micro F1 ', f1_score(y_true, y_predicted, average='micro'))"
      ],
      "metadata": {
        "id": "u8A8mMCyfztX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clfNB = MultinomialNB()\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaledTrainX= scaler.fit_transform(trainReviewVectors)\n",
        "scaledTestX = scaler.fit_transform(testReviewVectors)\n",
        "clfNB.fit(scaledTrainX, trainLabels)\n",
        "\n",
        "#test naive bayes accuracy\n",
        "testLabelsPredicted=list(clfNB.predict(scaledTestX))\n",
        "\n",
        "#print results\n",
        "print(\"####################RESULTS OF NAIVE BAYES CLASSIFIER##################\")\n",
        "printResults(testLabelsPredicted, testLabels)"
      ],
      "metadata": {
        "id": "nv0ISp8YfABY",
        "outputId": "c90787f5-0644-4d23-d410-38f7ccf83986",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####################RESULTS OF NAIVE BAYES CLASSIFIER##################\n",
            "Accuracy=  0.654\n",
            "###########################################\n",
            "precision: [0.6004902  0.70532915]\n",
            "recall: [0.66156616 0.64794816]\n",
            "fscore: [0.62955032 0.67542214]\n",
            "support: [1111 1389]\n",
            "###########################################3\n",
            "Macro F1  0.6524862300179582\n",
            "Micro F1  0.654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#neural network\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "clfMLP = MLPClassifier(hidden_layer_sizes=(10, 10, 10))\n",
        "clfMLP.fit(trainReviewVectors, trainLabels)\n",
        "  \n",
        "testLabelsPredicted=list(clfMLP.predict(testReviewVectors))\n",
        "\n",
        "#print results\n",
        "print(\"####################RESULTS OF NEURAL NETWORK CLASSIFIER##################\")\n",
        "printResults(testLabelsPredicted, testLabels)"
      ],
      "metadata": {
        "id": "2UAUyiDpgVZp",
        "outputId": "0a935e17-550c-4f4e-80b2-059faa74dc6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####################RESULTS OF NEURAL NETWORK CLASSIFIER##################\n",
            "Accuracy=  0.7824\n",
            "###########################################\n",
            "precision: [0.75245098 0.81112853]\n",
            "recall: [0.79259897 0.7735426 ]\n",
            "fscore: [0.77200335 0.79188982]\n",
            "support: [1162 1338]\n",
            "###########################################3\n",
            "Macro F1  0.7819465884581764\n",
            "Micro F1  0.7824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clfRF = RandomForestClassifier(n_estimators = 100)\n",
        "# Train the model on training data\n",
        "clfRF.fit(trainReviewVectors, trainLabels);\n",
        "\n",
        "testLabelsPredicted=list(clfRF.predict(testReviewVectors))\n",
        "\n",
        "#print results\n",
        "print(\"####################RESULTS OF Random Forest CLASSIFIER##################\")\n",
        "printResults(testLabelsPredicted, testLabels)"
      ],
      "metadata": {
        "id": "x7zUrGf0gaoQ",
        "outputId": "105e1a61-020b-4925-e93c-9b3628f08a3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####################RESULTS OF Random Forest CLASSIFIER##################\n",
            "Accuracy=  0.7588\n",
            "###########################################\n",
            "precision: [0.74264706 0.77429467]\n",
            "recall: [0.7593985  0.75825019]\n",
            "fscore: [0.75092937 0.76618845]\n",
            "support: [1197 1303]\n",
            "###########################################3\n",
            "Macro F1  0.7585589065817562\n",
            "Micro F1  0.7588\n"
          ]
        }
      ]
    }
  ]
}